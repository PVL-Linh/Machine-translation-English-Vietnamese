{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1723908861787,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"7TZazms9xsQz"},"outputs":[{"name":"stdout","output_type":"stream","text":["linhscscs\n"]}],"source":["print('linhscscs')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n","\n","Requirement already satisfied: numpy in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (1.26.4)\n","Requirement already satisfied: underthesea in c:\\users\\van linh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.8.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2024.1)\n","Requirement already satisfied: Click>=6.0 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from underthesea) (8.1.7)\n","Requirement already satisfied: python-crfsuite>=0.9.6 in c:\\users\\van linh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (0.9.10)\n","Requirement already satisfied: nltk in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from underthesea) (3.8.1)\n","Requirement already satisfied: tqdm in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from underthesea) (4.66.4)\n","Requirement already satisfied: requests in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from underthesea) (2.32.3)\n","Requirement already satisfied: joblib in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from underthesea) (1.4.2)\n","Requirement already satisfied: scikit-learn in c:\\users\\van linh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (1.5.1)\n","Requirement already satisfied: PyYAML in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from underthesea) (6.0.1)\n","Requirement already satisfied: underthesea-core==1.0.4 in c:\\users\\van linh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (1.0.4)\n","Requirement already satisfied: colorama in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from Click>=6.0->underthesea) (0.4.6)\n","Requirement already satisfied: six>=1.5 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from nltk->underthesea) (2024.5.15)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from requests->underthesea) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from requests->underthesea) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from requests->underthesea) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from requests->underthesea) (2024.2.2)\n","Requirement already satisfied: scipy>=1.6.0 in c:\\users\\van linh\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->underthesea) (1.13.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\van linh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->underthesea) (3.5.0)\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.0 -> 24.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["pip install pandas numpy underthesea"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723908865780,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"CcRMGf62FKTx"},"outputs":[{"ename":"OSError","evalue":"[WinError 127] The specified procedure could not be found","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munderthesea\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterable, List\n","File \u001b[1;32mc:\\Users\\Van Linh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n","File \u001b[1;32mc:\\Users\\Van Linh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Van Linh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n","File \u001b[1;32mc:\\Users\\Van Linh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Van Linh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_ops.py:1295\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1290\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m-> 1295\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n","File \u001b[1;32mc:\\Users\\Van Linh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ctypes\\__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n","\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found"]}],"source":["import pandas as pd\n","import numpy as np\n","import re,string\n","from underthesea import word_tokenize\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from typing import Iterable, List\n","#from gensim.models import KeyedVectors\n","from torch import Tensor\n","import torch\n","import torch.nn as nn\n","from torch.nn import Transformer\n","from torch.nn.utils.rnn import pad_sequence\n","from timeit import default_timer as timer\n","import math\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723908877908,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"4lWqsSUGFKTy"},"outputs":[],"source":["# data_dir = \"/content/drive/MyDrive/xlnntn/data/\"\n","# en_sents = open(data_dir + 'en_sents', \"r\",encoding=\"utf-8\").read().splitlines()\n","# vi_sents = open(data_dir + 'vi_sents', \"r\",encoding=\"utf-8\").read().splitlines()\n","# raw_data = {\n","#         \"en\": [line for line in en_sents[:254000]], # Only take first 170000 lines\n","#         \"vi\": [line for line in vi_sents[:254000]], # 254000\n","#     }\n","# df = pd.DataFrame(raw_data, columns=[\"en\", \"vi\"])\n","# print(len(en_sents))\n","# df.head()"]},{"cell_type":"markdown","metadata":{"id":"8VLND_rCFKTz"},"source":["This dataset provides a set of 254,090 tuples containing an English source sentence, its Vietnamese human translation and we take 170000 set for training and evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"elapsed":40096,"status":"ok","timestamp":1723908918000,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"sLtzN5kTlArs","outputId":"caa1568e-2e30-4172-b08d-b2b8172d0c97"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of English sentences: 4125698\n","Number of Vietnamese sentences: 4125865\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>en</th>\n","      <th>vi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I have to go to sleep.</td>\n","      <td>Tôi phải đi ngủ.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Today is June 18th and it is Muiriel's birthday!</td>\n","      <td>Hôm nay là ngày 18 tháng sáu, và cũng là ngày ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Muiriel is 20 now.</td>\n","      <td>Bây giờ Muiriel được 20 tuổi.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The password is \"Muiriel\".</td>\n","      <td>Mật mã là \"Muiriel\".</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I will be back soon.</td>\n","      <td>Tôi trở lại mau.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                 en  \\\n","0                            I have to go to sleep.   \n","1  Today is June 18th and it is Muiriel's birthday!   \n","2                                Muiriel is 20 now.   \n","3                        The password is \"Muiriel\".   \n","4                              I will be back soon.   \n","\n","                                                  vi  \n","0                                   Tôi phải đi ngủ.  \n","1  Hôm nay là ngày 18 tháng sáu, và cũng là ngày ...  \n","2                      Bây giờ Muiriel được 20 tuổi.  \n","3                               Mật mã là \"Muiriel\".  \n","4                                   Tôi trở lại mau.  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import os\n","\n","# Đường dẫn tới thư mục chứa dữ liệu\n","data_dir = \"data12/\"\n","\n","# Danh sách lưu trữ các câu\n","en_sents = []\n","vi_sents = []\n","\n","# Đọc dữ liệu từ các thư mục con\n","for subdir in os.listdir(data_dir):\n","    subdir_path = os.path.join(data_dir, subdir)\n","    if os.path.isdir(subdir_path):\n","        en_file = os.path.join(subdir_path, 'data.en')\n","        vi_file = os.path.join(subdir_path, 'data.vi')\n","\n","        if os.path.isfile(en_file):\n","            with open(en_file, \"r\", encoding=\"utf-8\") as file:\n","                en_sents.extend(file.read().splitlines())\n","\n","        if os.path.isfile(vi_file):\n","            with open(vi_file, \"r\", encoding=\"utf-8\") as file:\n","                vi_sents.extend(file.read().splitlines())\n","\n","# Đọc dữ liệu từ các tập tin train, dev, tst\n","for file_name in ['train.en', 'train.vi', 'dev.en', 'dev.vi', 'tst.en', 'tst.vi']:\n","    file_path = os.path.join(data_dir, file_name)\n","\n","    if file_name.endswith('.en'):\n","        if os.path.isfile(file_path):\n","            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","                en_sents.extend(file.read().splitlines())\n","\n","    elif file_name.endswith('.vi'):\n","        if os.path.isfile(file_path):\n","            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","                vi_sents.extend(file.read().splitlines())\n","\n","# Kiểm tra độ dài của các danh sách\n","print(f\"Number of English sentences: {len(en_sents)}\")\n","print(f\"Number of Vietnamese sentences: {len(vi_sents)}\")\n","\n","# Đảm bảo hai danh sách có cùng độ dài (nếu cần thiết)\n","min_length = min(len(en_sents), len(vi_sents))\n","en_sents = en_sents[:min_length]\n","vi_sents = vi_sents[:min_length]\n","\n","# Tạo DataFrame\n","raw_data = {\n","    \"en\": en_sents,\n","    \"vi\": vi_sents,\n","}\n","df = pd.DataFrame(raw_data, columns=[\"en\", \"vi\"])\n","\n","# Hiển thị thông tin\n","df.head()\n"]},{"cell_type":"markdown","metadata":{"id":"Nx5pD9KTFKT0"},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":147},"executionInfo":{"elapsed":571,"status":"ok","timestamp":1723908918568,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"nx_EZ6WcFKT1","outputId":"1a9e6df6-7d00-4cfa-b638-882410d48c6a"},"outputs":[{"data":{"text/plain":["en    0\n","vi    0\n","dtype: int64"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df.isna().sum()"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723908918568,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"u__1SMEyFKT1"},"outputs":[],"source":["# def preprocessing(df):\n","#   df[\"en\"] = df[\"en\"].apply(lambda ele: ele.translate(str.maketrans('', '', string.punctuation))) # Remove punctuation\n","#   df[\"vi\"] = df[\"vi\"].apply(lambda ele: ele.translate(str.maketrans('', '', string.punctuation)))\n","#   df[\"en\"] = df[\"en\"].apply(lambda ele: ele.lower()) # convert text to lowercase\n","#   df[\"vi\"] = df[\"vi\"].apply(lambda ele: ele.lower())\n","#   df[\"en\"] = df[\"en\"].apply(lambda ele: ele.strip())\n","#   df[\"vi\"] = df[\"vi\"].apply(lambda ele: ele.strip())\n","#   df[\"en\"] = df[\"en\"].apply(lambda ele: re.sub(\"\\s+\", \" \", ele))\n","#   df[\"vi\"] = df[\"vi\"].apply(lambda ele: re.sub(\"\\s+\", \" \", ele))\n","\n","#   return df\n","\n","# df = preprocessing(df)\n","# df.head()"]},{"cell_type":"markdown","metadata":{"id":"UaXPy_YrFKT2"},"source":["Tokenzing for english which is source language by tokenizer of basic english and vietnamese which is target language by Underthesea library"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":3734631,"status":"ok","timestamp":1723912653196,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"W79xSs1kFKT2"},"outputs":[],"source":["# Create source and target language tokenizer.\n","SRC_LANGUAGE = 'en'\n","TGT_LANGUAGE = 'vi'\n","\n","# Place-holders\n","token_transform = {}\n","vocab_transform = {}\n","\n","# Tokenize for vietnames by underthesea\n","def vi_tokenizer(sentence):\n","    tokens = word_tokenize(sentence)\n","    return tokens\n","\n","token_transform[SRC_LANGUAGE] = get_tokenizer('basic_english')\n","token_transform[TGT_LANGUAGE] = get_tokenizer(vi_tokenizer)\n","\n","# helper function to yield list of tokens\n","def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n","    for index,data_sample in data_iter:\n","        yield token_transform[language](data_sample[language])\n","\n","# Define special symbols and indices\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","# Make sure the tokens are in order of their indices to properly insert them in vocab\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n","\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    # Training data Iterator\n","    train_iter = df.iterrows()\n","    # Create torchtext's Vocab object\n","    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n","                                                    min_freq=1,\n","                                                    specials=special_symbols,\n","                                                    special_first=True)\n","\n","# Set UNK_IDX as the default index. This index is returned when the token is not found.\n","# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","  vocab_transform[ln].set_default_index(UNK_IDX)"]},{"cell_type":"markdown","metadata":{"id":"eYF6v8RBFKT3"},"source":["## Model Defination"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1723912653197,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"06RIM4yZFKT3"},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #Check whether running on gpu or cpu\n","\n","# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,\n","                 emb_size: int,\n","                 dropout: float = 0.1,\n","                 maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n","\n","# Seq2Seq Network\n","class Seq2SeqTransformer(nn.Module):\n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 num_decoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 src_vocab_size: int,\n","                 tgt_vocab_size: int,\n","                 dim_feedforward: int = 512,\n","                 dropout: float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        self.transformer = Transformer(d_model=emb_size,\n","                                       nhead=nhead,\n","                                       num_encoder_layers=num_encoder_layers,\n","                                       num_decoder_layers=num_decoder_layers,\n","                                       dim_feedforward=dim_feedforward,\n","                                       dropout=dropout)\n","        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n","        self.positional_encoding = PositionalEncoding(\n","            emb_size, dropout=dropout)\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                src_mask: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor,\n","                memory_key_padding_mask: Tensor):\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n","                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n","        return self.generator(outs)\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):\n","        return self.transformer.encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n","        return self.transformer.decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723912653197,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"uG_jy1M2FKT4"},"outputs":[],"source":["def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"]},{"cell_type":"markdown","metadata":{"id":"xK72XCE9FKT4"},"source":["Define the parameters of the model, instantiate the same and the loss function which is the cross-entropy loss and the optmizer used for training is Adam with β1 = 0.9, β2 = 0.98 and epsilon = 1e−9."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723912653197,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"Bwqo8evznRnX","outputId":"12edf0b6-ac9a-4168-bd65-f8aefc1e86af"},"outputs":[{"name":"stdout","output_type":"stream","text":["190216\n","343842\n"]}],"source":["print(len( vocab_transform[SRC_LANGUAGE]))\n","print(len( vocab_transform[TGT_LANGUAGE]))"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":16405,"status":"ok","timestamp":1723912669600,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"zpuy0SKaFKT4"},"outputs":[],"source":["torch.manual_seed(0)\n","SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n","TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n","EMB_SIZE = 512\n","NHEAD = 8 # embed_dim must be divisible by num_heads\n","FFN_HID_DIM = 512\n","BATCH_SIZE = 64\n","NUM_ENCODER_LAYERS = 4\n","NUM_DECODER_LAYERS = 4\n","DROP_OUT = 0.1\n","\n","transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n","                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM,DROP_OUT)\n","\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","transformer = transformer.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"]},{"cell_type":"markdown","metadata":{"id":"A_O_QaIIFKT5"},"source":["Define collate function that convert batch of raw strings into batch tensors that can be fed directly into model."]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3036,"status":"ok","timestamp":1723912672627,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"zGoK4JgSFKT5"},"outputs":[],"source":["# helper function to club together sequential operations\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# function to add BOS/EOS and create tensor for input sequence indices\n","def tensor_transform(token_ids: List[int]):\n","    return torch.cat((torch.tensor([BOS_IDX]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([EOS_IDX])))\n","\n","# src and tgt language text transforms to convert raw strings into tensors indices\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n","                                               vocab_transform[ln], #Numericalization\n","                                               tensor_transform) # Add BOS/EOS and create tensor\n","\n","\n","\n","\n","# function to collate data samples into batch tesors\n","def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return src_batch, tgt_batch"]},{"cell_type":"markdown","metadata":{"id":"FgJRQju0FKT5"},"source":["Split data to train and test set"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723912673227,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"8koBjvDfFKT5"},"outputs":[],"source":["# Split data to tran test set\n","split_ratio = 0.9\n","split = round(df.shape[0]* split_ratio)\n","train = df.iloc[:split]\n","train_ds = list(zip(train['en'],train['vi']))\n","valid = df.iloc[split:]\n","val_ds = list(zip(valid['en'],valid['vi']))"]},{"cell_type":"markdown","metadata":{"id":"iToKpXaQFKT5"},"source":["Define training and evaluation loop that will be called for each epoch with Gradient accumulation which is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1723912673227,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"Hlb0NBbWFKT5"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","accumulation_steps = 5\n","\n","def train_epoch(model, optimizer):\n","    model.train()\n","    losses = 0\n","    val_los = 0\n","    train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","    optimizer.zero_grad()\n","    for i, (src, tgt) in enumerate(train_dataloader):\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss = loss / accumulation_steps # Normalize our loss (if averaged)\n","        loss.backward()\n","\n","        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n","            optimizer.step() # Now we can do an optimizer step\n","            optimizer.zero_grad() # Reset gradients tensor\n","\n","        losses += loss.item()\n","\n","    return losses / len(train_dataloader)\n","\n","def evaluate(model):\n","    model.eval()\n","    losses = 0\n","\n","    #val_iter = valid.iterrows()\n","    val_dataloader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in val_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss = loss / accumulation_steps # Normalize our loss (if averaged)\n","        losses += loss.item()\n","\n","    return losses / len(val_dataloader)"]},{"cell_type":"markdown","metadata":{"id":"5E4qGVTpFKT6"},"source":["Define an early stopping function to avoid the model from overfit"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1723912673227,"user":{"displayName":"Linh Phạm Văn","userId":"17103109700858367777"},"user_tz":-420},"id":"o505HyIfFKT6"},"outputs":[],"source":["class EarlyStopping():\n","    def __init__(self, tolerance=5, min_delta=0):\n","\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.early_stop = False\n","\n","    def __call__(self, train_loss, validation_loss):\n","        if (validation_loss - train_loss) > self.min_delta:\n","            self.counter +=1\n","            if self.counter >= self.tolerance:\n","                self.early_stop = True"]},{"cell_type":"markdown","metadata":{"id":"Hfra3L6wFKT6"},"source":["Training model"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"eBQieFHlFKT6"},"outputs":[{"ename":"NameError","evalue":"name 'EarlyStopping' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m \u001b[43mEarlyStopping\u001b[49m(tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      2\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_los\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m      6\u001b[0m         }\n","\u001b[1;31mNameError\u001b[0m: name 'EarlyStopping' is not defined"]}],"source":["early_stopping = EarlyStopping(tolerance=5, min_delta=0.1)\n","NUM_EPOCHS = 3\n","history = {\n","        \"loss\": [],\n","        \"val_los\": []\n","        }\n","\n","for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = timer()\n","    train_loss = train_epoch(transformer, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(transformer)\n","    history['loss'].append(train_loss)\n","    history['val_los'].append(val_loss)\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n","    # Early Stopping\n","    early_stopping(train_loss, val_loss)\n","    if early_stopping.early_stop:\n","        print(\"We are at epoch:\", epoch)\n","        break"]},{"cell_type":"markdown","metadata":{"id":"afBgcR-bFKT6"},"source":["Traning and Validate plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbaFPl5iFKT7"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.plot(history['loss'], label = \"loss\")\n","plt.plot(history['val_los'], label = \"Val loss\")\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.title('Loss vs. No. of epochs')"]},{"cell_type":"markdown","metadata":{"id":"4DVsQqEoFKT7"},"source":["**Inference**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5narhjx0FKT7"},"outputs":[],"source":["# function to generate output sequence using greedy algorithm\n","def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","\n","# actual function to translate input sentence into target language\n","def translate(model: torch.nn.Module, src_sentence: str):\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = greedy_decode(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmCuUIuoFKT7"},"outputs":[],"source":["# Saving model\n","torch.save(transformer.state_dict(), \"viEn_transformer1.pth\")"]},{"cell_type":"markdown","metadata":{"id":"OhFPuE4pFKT7"},"source":["Testing model by randomly take 10 samples from the rest of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ri57m73FFKT8"},"outputs":[],"source":["from random import randint\n","rand = [randint(170000,200000) for i in range(10)]\n","test_set = [\n","        [en_sents[i] for i in rand],\n","        [vi_sents[i] for i in rand]]\n","\n","for i in range(len(test_set[0])):\n","    print('Input English sentence:', test_set[0][i])\n","    print('Actual Vietnamese Translation:', test_set[1][i])\n","    print('Predicted Vietnamese Translation:', translate(transformer, test_set[0][i]))\n","    print(\"====> \" , translate(transformer, 'end the war'))\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASYvrsIuFF81"},"outputs":[],"source":["print(\"====> \" , translate(transformer, 'I am Linh, who are you?'))"]},{"cell_type":"markdown","metadata":{"id":"v3L-MWANdnIy"},"source":["First Segment:\n","\"The sun was setting over the horizon, casting a golden glow across the sky. The gentle breeze rustled through the leaves of the trees. Birds were returning to their nests as the day turned into night.\"\n","\n","Second Segment:\n","\"In the quiet of the evening, the sounds of the city began to fade away. The streetlights flickered on, one by one, illuminating the sidewalks. People started to head home, ending their day.\"\n","\n","Third Segment:\n","\"As the stars began to appear, the night sky became a canvas of twinkling lights. The tranquility of the night settled in, bringing a sense of calm. It was the perfect end to a beautiful day.\"\n","\n","Đoạn văn tiếng Việt:\n","Đoạn đầu tiên:\n","\"Mặt trời đang lặn dần về phía chân trời, ánh sáng vàng óng ánh lên bầu trời. Cơn gió nhẹ thổi qua những tán lá của cây cối. Những chú chim trở về tổ khi ngày chuyển dần thành đêm.\"\n","\n","Đoạn thứ hai:\n","\"Trong sự yên tĩnh của buổi tối, âm thanh của thành phố bắt đầu lắng xuống. Những đèn đường lần lượt sáng lên, chiếu sáng vỉa hè. Mọi người bắt đầu trở về nhà, kết thúc một ngày làm việc.\"\n","\n","Đoạn thứ ba:\n","\"Khi những vì sao bắt đầu xuất hiện, bầu trời đêm trở thành một bức tranh ánh sáng lấp lánh. Sự yên bình của đêm buông xuống, mang đến cảm giác thư thái. Đó là cái kết hoàn hảo cho một ngày đẹp trời.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BaFBLuAF7L5_"},"outputs":[],"source":["import re\n","\n","def split_text(text):\n","    # Tách văn bản thành các phần trước và sau ký tự đặc biệt, bao gồm ký tự đặc biệt\n","    parts = re.split(r'([,.!?;:])', text)\n","    return parts\n","\n","\n","def process_parts(parts):\n","    processed_parts = []\n","\n","    # Vòng lặp qua từng phần của văn bản đã tách\n","    for part in parts:\n","\n","        # Ở đây có thể thêm bất kỳ xử lý nào bạn cần\n","        if part in [',','.','!','?',';',':']:\n","          processed_parts.append(part)\n","        elif part ==  \"\":\n","          processed_parts.append(part)\n","        else :\n","          part = translate(transformer, part)\n","          processed_parts.append(part)\n","\n","\n","\n","    # Ghép các phần lại với nhau\n","    result = ''.join(processed_parts)\n","    return result\n","\n","# Ví dụ\n","text = \"In the quiet of the evening, the sounds of the city began to fade away. The streetlights flickered on, one by one, illuminating the sidewalks. People started to head home, ending their day.\"\n","parts = split_text(text)  # Tách văn bản\n","result = process_parts(parts)  # Xử lý các phần đã tách\n","print(result)  # Output: \"I am Linh, who are you? Let's go!\"\n","out = \"Trong sự yên tĩnh của buổi tối, âm thanh của thành phố bắt đầu lắng xuống. Những đèn đường lần lượt sáng lên, chiếu sáng vỉa hè. Mọi người bắt đầu trở về nhà, kết thúc một ngày làm việc.\"\n","print(out)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKB8lsk3XsBX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOjKtKhc5bCpjaf8zVmPSc0","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
